{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89077e9-add9-40c5-84fc-701287281e57",
   "metadata": {},
   "source": [
    "### (Execute if your running in Binder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ae3cf-a18b-4142-a3d6-190fb46dbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install openpyxl\n",
    "# !pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fab08c-04ae-4b25-b88d-5c291faa740f",
   "metadata": {},
   "source": [
    "# HS Conversion Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256d55eb-f944-4bff-931f-28ee2f4fb723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Desktop\\CEP\\HSConversionTables\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a31f0d1-22b1-41ec-bcb6-f82fe5bd51bc",
   "metadata": {},
   "source": [
    "## Get Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25303bf0-e056-4b9d-84b5-ea581d4d72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HSTABLES = \"https://unstats.un.org/unsd/trade/classifications/tables/CompleteCorrelationsOfHS-SITC-BEC_20170606.xlsx\"\n",
    "HS = ['HS92', 'HS96', 'HS02', 'HS07', 'HS12', 'HS17'] #should be updated if changed\n",
    "\n",
    "import functools\n",
    "from datetime import datetime\n",
    "\n",
    "def return_used_versions(start_year = None, end_year=None):\n",
    "    if start_year==None: \n",
    "        start_year = 1992\n",
    "    if end_year==None: \n",
    "        end_year = datetime.now().year\n",
    "    if start_year < 1992 or end_year<start_year or end_year>datetime.now().year: \n",
    "        print(\"Uncorrect years\")\n",
    "    else: \n",
    "        years = [1992, 1996, 2002, 2007, 2012, 2017]\n",
    "        versions = ['HS92', 'HS96', 'HS02', 'HS07', 'HS12', 'HS17']\n",
    "        version_dict = {k:v for k,v in zip(years, versions)}\n",
    "        start_v = functools.reduce(lambda a,b: a if a<=start_year else b, list(reversed(years)))\n",
    "        end_v = functools.reduce(lambda a,b: a if a<=end_year else b, list(reversed(years)))\n",
    "        years_list = years[years.index(start_v):years.index(end_v)+1]\n",
    "        return [version_dict[x] for x in years_list]\n",
    "\n",
    "def find_homogeneous_serie(position=None, start_year=None, end_year=None):\n",
    "    if position: \n",
    "        cols = return_used_versions(start_year, end_year)\n",
    "        print(f\"Loading HS Correlations Tables\\n\")\n",
    "        data = pd.read_excel(HSTABLES,\n",
    "                            usecols= HS,\n",
    "                            dtype = {v:'object' for v in HS}).dropna()\n",
    "        data = data[cols].drop_duplicates()\n",
    "        versions = [v for v in cols if data[v].str.contains(position).any()]\n",
    "        for col in cols:\n",
    "            data[col] = data[col].apply(lambda x: col+\"-\"+str(int(x)).zfill(6))\n",
    "        connections = []\n",
    "        for i in range(len(cols)-1): \n",
    "            temp_tup_list = list(data[[cols[i], cols[i+1]]].drop_duplicates().itertuples(index=False, name=None))\n",
    "            connections = connections + temp_tup_list \n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(connections)\n",
    "        positions = sorted(nx.node_connected_component(G, versions[-1]+\"-\"+position))\n",
    "        print(positions)\n",
    "        #return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb8b3e6-d6b7-46aa-aade-3ae925c094ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HS Correlations Tables\n",
      "\n",
      "['HS02-010110', 'HS02-010190', 'HS07-010110', 'HS07-010190', 'HS12-010121', 'HS12-010129', 'HS12-010130', 'HS12-010190', 'HS17-010121', 'HS17-010129', 'HS17-010130', 'HS17-010190', 'HS92-010111', 'HS92-010119', 'HS92-010120', 'HS96-010111', 'HS96-010119', 'HS96-010120']\n"
     ]
    }
   ],
   "source": [
    "find_homogeneous_serie(\"010111\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a0a31-2800-48c3-b719-888ac23a4dc8",
   "metadata": {},
   "source": [
    "## Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd6b52c6-a651-455f-b03d-56c95bf32c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "HSTABLES = \"https://unstats.un.org/unsd/trade/classifications/tables/CompleteCorrelationsOfHS-SITC-BEC_20170606.xlsx\"\n",
    "\n",
    "HS = ['HS92', 'HS96', 'HS02', 'HS07', 'HS12', 'HS17'] #should be updated if changed\n",
    "\n",
    "def return_used_versions(start_year = None, end_year=None):\n",
    "    years = [1992, 1996, 2002, 2007, 2012, 2017]\n",
    "    versions = HS\n",
    "    version_dict = {k:v for k,v in zip(years, versions)}\n",
    "    start_v = functools.reduce(lambda a,b: a if a<=start_year else b, list(reversed(years)))\n",
    "    end_v = functools.reduce(lambda a,b: a if a<=end_year else b, list(reversed(years)))\n",
    "    years_list = years[years.index(start_v):years.index(end_v)+1]\n",
    "    return [version_dict[x] for x in years_list]\n",
    "\n",
    "def recursive_trade_off(df, position, k): \n",
    "    \n",
    "    if k>=0:\n",
    "        currcols = df.columns.tolist()[k:]\n",
    "        print(f\"Evaluating graph object with {','.join(currcols)} versions\\n\")\n",
    "        data = df[currcols].drop_duplicates()\n",
    "        connections = []\n",
    "        for i in range(len(currcols)-1): \n",
    "            temp_tup_list = list(data[[currcols[i], currcols[i+1]]].drop_duplicates().itertuples(index=False, name=None))\n",
    "            connections = connections + temp_tup_list\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(connections)\n",
    "        try:\n",
    "            positions = sorted(nx.node_connected_component(G, currcols[-1]+\"-\"+position))\n",
    "            if len(positions)==len(currcols): \n",
    "                return recursive_trade_off(df, position, k-1)\n",
    "            else: \n",
    "                print(\"Evaluation finished\\n\")\n",
    "                print(f\"The {','.join(currcols[1:])} versions support your position with no precision loss\")\n",
    "        except:\n",
    "            print(f\"Position {position} not founded\")\n",
    "    else:\n",
    "        print(f\"The {','.join(df.columns.tolist())} versions support your position with no precision loss\")\n",
    "        \n",
    "    \n",
    "def trade_off(position=None, start_year=None, end_year=None):\n",
    "    if start_year==None: \n",
    "        start_year = 1992\n",
    "    if end_year==None: \n",
    "        end_year = datetime.now().year\n",
    "    if start_year < 1992 or end_year<start_year or end_year>datetime.now().year: \n",
    "        print(\"Uncorrect years\")\n",
    "    \n",
    "    if position: \n",
    "        versions = return_used_versions(start_year, end_year)\n",
    "        print(f\"Period between {start_year} and {end_year} contains the {','.join(versions)} versions\\n\") \n",
    "        print(f\"Loading HS Correlations Tables\\n\")\n",
    "        \n",
    "        data = pd.read_excel(HSTABLES,\n",
    "                            usecols= HS,\n",
    "                            dtype = {v:'object' for v in HS}).dropna()\n",
    "        versions = [v for v in versions if data[v].str.contains(position).any()]\n",
    "        \n",
    "        if len(versions)>0: \n",
    "            print(f\"The position {position} was included in the {','.join(versions)} versions\\n\")\n",
    "            data = data[versions].drop_duplicates()\n",
    "            for col in data.columns:\n",
    "                data[col] = data[col].apply(lambda x: col+\"-\"+str(int(x)).zfill(6))\n",
    "            k=len(versions)-2\n",
    "            print(f\"Evaluating maximum period with no precision loss for position {position}\\n\")\n",
    "            return recursive_trade_off(data, position, k)\n",
    "        else:\n",
    "            print(f\"Position {position} not founded in that period\")\n",
    "    else:\n",
    "        print(\"Please define a position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a685f9-1771-4821-b460-885236241a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period between 1992 and 2022 contains the HS92,HS96,HS02,HS07,HS12,HS17 versions\n",
      "\n",
      "Loading HS Correlations Tables\n",
      "\n",
      "The position 010130 was included in the HS12,HS17 versions\n",
      "\n",
      "Evaluating maximum period with no precision loss for position 010130\n",
      "\n",
      "Evaluating graph object with HS12,HS17 versions\n",
      "\n",
      "The HS12,HS17 versions support your position with no precision loss\n"
     ]
    }
   ],
   "source": [
    "trade_off(\"010130\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
