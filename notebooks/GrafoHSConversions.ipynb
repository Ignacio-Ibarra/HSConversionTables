{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fab08c-04ae-4b25-b88d-5c291faa740f",
   "metadata": {},
   "source": [
    "# HS Conversion Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256d55eb-f944-4bff-931f-28ee2f4fb723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206108c1-fa38-4e4a-9f5a-3775125a379d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### En desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f498435-6a3b-4de8-a19d-885d425b06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import functools\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from operator import and_\n",
    "\n",
    "\n",
    "class HSCorrelations: \n",
    "    \n",
    "    def __init__(self, path=None):\n",
    "        self.__HS = ['HS92', 'HS96', 'HS02', 'HS07', 'HS12', 'HS17'] #should be updated if changed \n",
    "        print(f\"Loading HS Correlation Tables from path provided\")\n",
    "        if path: \n",
    "            self.__data = pd.read_excel(path,\n",
    "                            usecols= self.__HS,\n",
    "                            dtype = {v:'object' for v in self.__HS}).dropna().drop_duplicates()\n",
    "        else:\n",
    "            print(\"Loading HS Correlation Tables from UNSTATS\\n server\")\n",
    "            self.__url = \"https://unstats.un.org/unsd/trade/classifications/tables/CompleteCorrelationsOfHS-SITC-BEC_20170606.xlsx\"\n",
    "            self.__data = pd.read_excel(self.__url,\n",
    "                                usecols= self.__HS,\n",
    "                                dtype = {v:'object' for v in self.__HS}).dropna().drop_duplicates()\n",
    "        print(\"Tables already loaded. Use `available_methods()` for more information\\n\")\n",
    "        \n",
    "    def available_methods(self):\n",
    "        print(\"Use `get_df()` to get dataframe with the correlations between all HS versions\\n\")\n",
    "        print(\"Use `get_codes()` to get the codes by: version (default HS17, str or list) or chapter\")\n",
    "        print(\"Use `check_position()` to check if position exists\")\n",
    "        print(\"Use `year_to_HS()` to get the available HS version from years\")\n",
    "        #uncomplete\n",
    "     \n",
    "    def get_df(self):\n",
    "        return self.__data\n",
    "    \n",
    "    def get_url(self):\n",
    "        return self.__url\n",
    "    \n",
    "    def get_versions(self): \n",
    "        return self.__HS\n",
    "\n",
    "    def get_codes(self, version=\"HS17\", chapter=None): \n",
    "        df = pd.DataFrame(self.__data[version], columns = [version]).drop_duplicates()\n",
    "        if chapter:\n",
    "            df = df[df[version].str.startswith(chapter)]\n",
    "        return df\n",
    "    \n",
    "    def check_position(self, position=None):\n",
    "        if position:\n",
    "            return position in self.__data.values\n",
    "            \n",
    "    \n",
    "    def year_to_HS(self, start_year=None, end_year=None):\n",
    "        if start_year==None: \n",
    "            start_year = 1992\n",
    "        if end_year==None: \n",
    "            end_year = datetime.now().year\n",
    "        if start_year < 1992 or end_year<start_year or end_year>datetime.now().year: \n",
    "            print(\"Uncorrect years\")\n",
    "        years = [1992, 1996, 2002, 2007, 2012, 2017]\n",
    "        version_dict = {k:v for k,v in zip(years, self.__HS)}\n",
    "        start_v = functools.reduce(lambda a,b: a if a<=start_year else b, list(reversed(years)))\n",
    "        end_v = functools.reduce(lambda a,b: a if a<=end_year else b, list(reversed(years)))\n",
    "        years_list = years[years.index(start_v):years.index(end_v)+1]\n",
    "        return [version_dict[x] for x in years_list]\n",
    "    \n",
    "    def HS_to_years(self,hslist):\n",
    "        y = [1992, 1996, 2002, 2007, 2012, 2017]\n",
    "        v_dict = {k:v for k,v in zip(self.__HS, y)}\n",
    "        y_list = [v_dict[x] for x in hslist]\n",
    "        start = min(y_list)\n",
    "        end = datetime.now().year if max(y_list)==y[-1] else max(y_list)\n",
    "        if start==end:\n",
    "            return f\"from {str(end)} to the present\"\n",
    "        else: \n",
    "            return f\"from {str(start)} to {str(end)}\"\n",
    "        \n",
    "        \n",
    "    def filter_df(self, positions=None, start_year=None, end_year=None): \n",
    "        \n",
    "        cols = self.year_to_HS(start_year, end_year)\n",
    "        df = self.__data[cols]\n",
    "        if positions:\n",
    "            filters = [(df[col].isin(positions)) for col in cols]\n",
    "            mask = functools.reduce(and_, filters, True)\n",
    "            return df[mask].drop_duplicates().reset_index(drop=True)\n",
    "        else:\n",
    "            return df         \n",
    "            \n",
    "    def find_homogeneous_serie(self, position=None, start_year=None, end_year=None):\n",
    "        if self.check_position(position): \n",
    "            cols = self.year_to_HS(start_year, end_year)\n",
    "            data = self.filter_df(start_year, end_year)\n",
    "            versions = [v for v in cols if data[v].str.contains(position).any()]\n",
    "            for col in cols:\n",
    "                data[col] = data[col].apply(lambda x: col+\"-\"+str(int(x)).zfill(6))\n",
    "            connections = []\n",
    "            for i in range(len(cols)-1): \n",
    "                temp_tup_list = list(data[[cols[i], cols[i+1]]].drop_duplicates().itertuples(index=False, name=None))\n",
    "                connections = connections + temp_tup_list \n",
    "            G = nx.Graph()\n",
    "            G.add_edges_from(connections)\n",
    "            positions = sorted(nx.node_connected_component(G, versions[-1]+\"-\"+position))\n",
    "            return positions\n",
    "        else:\n",
    "            print(\"Please define a correct position\")\n",
    "    \n",
    "    def genSankey(self, position=None, start_year=None, end_year=None, output_title='Sankey Diagram'):\n",
    "        if self.check_position(position): \n",
    "            related = list(set([x[5:] for x in self.find_homogeneous_serie(position, start_year, end_year)]))\n",
    "            cat_cols = self.year_to_HS(start_year, end_year)\n",
    "            df = self.filter_df(related, start_year, end_year)\n",
    "            df['count'] = 1\n",
    "            for col in cat_cols: \n",
    "                df[col] = df[col].apply(lambda x: col+\"-\"+str(int(x)).zfill(6))\n",
    "            colorPalette = ['#4B8BBE','#306998','#FFE873','#FFD43B','#646464',\"#002060\"]\n",
    "            labelList = []\n",
    "            colorNumList = []\n",
    "            for catCol in cat_cols:\n",
    "                labelListTemp =  list(set(df[catCol].values))\n",
    "                colorNumList.append(len(labelListTemp))\n",
    "                labelList = labelList + labelListTemp\n",
    "\n",
    "            # remove duplicates from labelList\n",
    "            labelList = list(dict.fromkeys(labelList))\n",
    "\n",
    "            # define colors based on number of levels\n",
    "            colorList = []\n",
    "            for idx, colorNum in enumerate(colorNumList):\n",
    "                colorList = colorList + [colorPalette[idx]]*colorNum\n",
    "\n",
    "            # transform df into a source-target pair\n",
    "            for i in range(len(cat_cols)-1):\n",
    "                if i==0:\n",
    "                    sourceTargetDf = df[[cat_cols[i],cat_cols[i+1],'count']]\n",
    "                    sourceTargetDf.columns = ['source','target','count']\n",
    "                else:\n",
    "                    tempDf = df[[cat_cols[i],cat_cols[i+1],'count']]\n",
    "                    tempDf.columns = ['source','target','count']\n",
    "                    sourceTargetDf = pd.concat([sourceTargetDf,tempDf])\n",
    "\n",
    "            sourceTargetDf = sourceTargetDf.groupby(['source','target']).agg({'count':'sum'}).reset_index()\n",
    "\n",
    "            # add index for source-target pair\n",
    "            sourceTargetDf['sourceID'] = sourceTargetDf['source'].apply(lambda x: labelList.index(x))\n",
    "            sourceTargetDf['targetID'] = sourceTargetDf['target'].apply(lambda x: labelList.index(x))\n",
    "\n",
    "            # creating the sankey diagram\n",
    "            data = dict(\n",
    "                type='sankey',\n",
    "                node = dict(\n",
    "                  pad = 15,\n",
    "                  thickness = 20,\n",
    "                  line = dict(\n",
    "                    color = \"black\",\n",
    "                    width = 0.5\n",
    "                  ),\n",
    "                  label = labelList,\n",
    "                  color = colorList\n",
    "                ),\n",
    "                link = dict(\n",
    "                  source = sourceTargetDf['sourceID'],\n",
    "                  target = sourceTargetDf['targetID'],\n",
    "                  value = sourceTargetDf['count']\n",
    "                )\n",
    "              )\n",
    "\n",
    "            layout =  dict(\n",
    "                title = output_title,\n",
    "                font = dict(\n",
    "                  size = 15\n",
    "                )\n",
    "            )\n",
    "\n",
    "            fig = dict(data=[data], layout=layout)\n",
    "            return fig\n",
    "        else: \n",
    "           print(\"Please define a correct position\") \n",
    "    \n",
    "    def recursive_trade_off(self, df, position, k): \n",
    "    \n",
    "        if k>=0:\n",
    "            currcols = df.columns.tolist()[k:]\n",
    "            print(f\"Evaluating graph object with {','.join(currcols)} versions\\n\")\n",
    "            data = df[currcols].drop_duplicates()\n",
    "            connections = []\n",
    "            for i in range(len(currcols)-1): \n",
    "                temp_tup_list = list(data[[currcols[i], currcols[i+1]]].drop_duplicates().itertuples(index=False, name=None))\n",
    "                connections = connections + temp_tup_list\n",
    "            G = nx.Graph()\n",
    "            G.add_edges_from(connections)\n",
    "            try:\n",
    "                positions = sorted(nx.node_connected_component(G, currcols[-1]+\"-\"+position))\n",
    "                if len(positions)==len(currcols): \n",
    "                    return self.recursive_trade_off(df, position, k-1)\n",
    "                else: \n",
    "                    print(\"Evaluation finished\\n\")\n",
    "                    print(f\"Your position has no precision loss {self.HS_to_years(currcols[1:])}\")\n",
    "            except:\n",
    "                print(f\"Position {position} not founded\")\n",
    "        else:\n",
    "            print(f\"Your position has no precision loss {self.HS_to_years(df.columns)}\")\n",
    "        \n",
    "    def trade_off(self, position=None, start_year=None, end_year=None):\n",
    "        if start_year==None: \n",
    "            start_year = 1992\n",
    "        if end_year==None: \n",
    "            end_year = datetime.now().year\n",
    "        if start_year < 1992 or end_year<start_year or end_year>datetime.now().year: \n",
    "            return print(\"Uncorrect years\")\n",
    "        if start_year>=2017:\n",
    "            return print(\"Both years belong to the latest version\")\n",
    "        if self.check_position(position): \n",
    "            versions = self.year_to_HS(start_year, end_year)\n",
    "            print(f\"Period between {start_year} and {end_year} contains the {','.join(versions)} versions\\n\") \n",
    "            print(f\"Loading HS Correlations Tables\\n\")\n",
    "            versions = [v for v in versions if self.__data[v].str.contains(position).any()]\n",
    "\n",
    "            if len(versions)>0: \n",
    "                print(f\"The position {position} was included in the {','.join(versions)} versions\\n\")\n",
    "                data = self.__data[versions].drop_duplicates()\n",
    "                for col in data.columns:\n",
    "                    data[col] = data[col].apply(lambda x: col+\"-\"+str(int(x)).zfill(6))\n",
    "                k=len(versions)-2\n",
    "                print(f\"Evaluating maximum period with no precision loss for position {position}\\n\")\n",
    "                return self.recursive_trade_off(data, position, k)\n",
    "            else:\n",
    "                print(f\"Position {position} not founded in that period\")\n",
    "        else:\n",
    "            print(\"Please define a correct position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "186272f4-1b9a-41de-be74-eaa1ffad6728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HS Correlation Tables from path provided\n",
      "Tables already loaded. Use `available_methods()` for more information\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullpath = 'C:\\\\Users\\\\Usuario\\\\Desktop\\\\CEP\\\\HSConversionTables\\\\data\\\\CompleteCorrelationsOfHS-SITC-BEC_20170606.xlsx'\n",
    "a = HSCorrelations(path = fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bcc7627b-efaf-44ca-b81f-9bf09ba6145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(a.check_position(\"010130\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cc5b01a-5280-4e18-a4d4-c60f21c9b53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     HS12\n",
       "2  010130"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_codes(version=\"HS12\", chapter=\"010130\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "37614c77-7a28-43dd-8470-f4fe680d4ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HS92', 'HS96', 'HS02', 'HS07', 'HS12', 'HS17']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d87ab62-b17c-4dca-866a-4c42dea2ba9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HS02', 'HS07', 'HS12', 'HS17']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.year_to_HS(2006,2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ab22063-06f1-4c04-a32e-74787d80c26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS92</th>\n",
       "      <th>HS96</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010111</td>\n",
       "      <td>010111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010111</td>\n",
       "      <td>010111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     HS92    HS96\n",
       "0  010111  010111\n",
       "1  010111  010111"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.filter_df([\"010111\"], 1993, 1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b9731e75-2ddf-4f2b-9443-b79fc95a1cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HS02-010110',\n",
       " 'HS02-010190',\n",
       " 'HS07-010110',\n",
       " 'HS07-010190',\n",
       " 'HS12-010121',\n",
       " 'HS12-010129',\n",
       " 'HS12-010130',\n",
       " 'HS12-010190',\n",
       " 'HS17-010121',\n",
       " 'HS17-010129',\n",
       " 'HS17-010130',\n",
       " 'HS17-010190',\n",
       " 'HS92-010111',\n",
       " 'HS92-010119',\n",
       " 'HS92-010120',\n",
       " 'HS96-010111',\n",
       " 'HS96-010119',\n",
       " 'HS96-010120']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.find_homogeneous_serie(\"010130\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b46611b5-944f-403d-8b88-bc7e6531721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import chart_studio.plotly as py\n",
    "import plotly\n",
    "\n",
    "fig = a.genSankey(\"010130\")\n",
    "if fig is not None:\n",
    "    plotly.offline.plot(fig, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a37ac887-4787-4d45-a357-9b050752b599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period between 2016 and 2019 contains the HS12,HS17 versions\n",
      "\n",
      "Loading HS Correlations Tables\n",
      "\n",
      "The position 010130 was included in the HS12,HS17 versions\n",
      "\n",
      "Evaluating maximum period with no precision loss for position 010130\n",
      "\n",
      "Evaluating graph object with HS12,HS17 versions\n",
      "\n",
      "Your position has no precision loss from 2012 to 2022\n"
     ]
    }
   ],
   "source": [
    "a.trade_off(\"010130\", 2016, 2019) #corregir que devuelva los años posibles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
