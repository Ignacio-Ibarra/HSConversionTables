{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebbfe477-f426-4238-9f2e-1a3d6a34c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nachengue/Escritorio/CEP/HSConversionTables\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import platform\n",
    "\n",
    "#Directorio\n",
    "directory = \"HSConversionTables\"\n",
    "if platform.system()=='Windows':\n",
    "    Path('C:/Users/Asus/Desktop/CEP/pedidos Igal/'+directory).mkdir(parents=True, exist_ok=True)\n",
    "    os.chdir('C:/Users/Asus/Desktop/CEP/pedidos Igal'+directory)\n",
    "else: \n",
    "    Path('/home/nachengue/Escritorio/CEP/'+directory).mkdir(parents=True, exist_ok=True)\n",
    "    os.chdir('/home/nachengue/Escritorio/CEP/'+directory)\n",
    "print(os.getcwd())\n",
    "\n",
    "#Data\n",
    "data = pd.read_excel(\"./data/CompleteCorrelationsOfHS-SITC-BEC_20170606.xlsx\")\n",
    "cols = ['HS92','HS96','HS02','HS07','HS12','HS17']\n",
    "data = data[cols].dropna().drop_duplicates()\n",
    "for col in cols: \n",
    "    data[col] = data[col].apply(lambda x: col+\"-\"+str(int(x)).zfill(6))\n",
    "data['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2a3c7b7-9dca-491e-aa41-11b56f58302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_df(df,cat_cols=[],value_cols=''):\n",
    "    \n",
    "    labelList = []\n",
    "    \n",
    "    for catCol in cat_cols:\n",
    "        labelListTemp =  df[catCol].unique().tolist()\n",
    "        colorNumList.append(len(labelListTemp))\n",
    "        labelList = labelList + labelListTemp\n",
    "        \n",
    "    # remove duplicates from labelList\n",
    "    labelList = list(dict.fromkeys(labelList))\n",
    "    \n",
    "         \n",
    "    # transform df into a source-target pair\n",
    "    for i in range(len(cat_cols)-1):\n",
    "        if i==0:\n",
    "            sourceTargetDf = df[[cat_cols[i],cat_cols[i+1],value_cols]]\n",
    "            sourceTargetDf.columns = ['source','target','count']\n",
    "        else:\n",
    "            tempDf = df[[cat_cols[i],cat_cols[i+1],value_cols]]\n",
    "            tempDf.columns = ['source','target','count']\n",
    "            sourceTargetDf = pd.concat([sourceTargetDf,tempDf])\n",
    "    \n",
    "    sourceTargetDf = sourceTargetDf.groupby(['source','target']).agg({'count':'sum'}).reset_index()\n",
    "        \n",
    "    # add index for source-target pair\n",
    "    sourceTargetDf['sourceID'] = sourceTargetDf['source'].apply(lambda x: labelList.index(x)+1)\n",
    "    sourceTargetDf['targetID'] = sourceTargetDf['target'].apply(lambda x: labelList.index(x)+1)\n",
    "\n",
    "    return sourceTargetDf\n",
    "\n",
    "\n",
    "def return_df2(df,cat_cols=[]):\n",
    "    \n",
    "    labelList = []\n",
    "    \n",
    "    for catCol in cat_cols:\n",
    "        labelListTemp =  df[catCol].unique().tolist()\n",
    "        labelList = labelList + labelListTemp\n",
    "        \n",
    "    # remove duplicates from labelList\n",
    "    labelList = list(dict.fromkeys(labelList))\n",
    "    \n",
    "         \n",
    "    # transform df into a source-target pair\n",
    "    for i in range(len(cat_cols)-1):\n",
    "        if i==0:\n",
    "            sourceTargetDf = df[[cat_cols[i],cat_cols[i+1]]]\n",
    "            sourceTargetDf.columns = ['source','target']\n",
    "        else:\n",
    "            tempDf = df[[cat_cols[i],cat_cols[i+1]]]\n",
    "            tempDf.columns = ['source','target']\n",
    "            sourceTargetDf = pd.concat([sourceTargetDf,tempDf])\n",
    "    \n",
    "    sourceTargetDf = sourceTargetDf.drop_duplicates()\n",
    "        \n",
    "    # add index for source-target pair starting from 1\n",
    "    sourceTargetDf['sourceID'] = sourceTargetDf['source'].apply(lambda x: labelList.index(x)+1)\n",
    "    sourceTargetDf['targetID'] = sourceTargetDf['target'].apply(lambda x: labelList.index(x)+1)\n",
    "\n",
    "    return sourceTargetDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25258d45-6cfb-4992-ab81-f05e7ebba251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HS17-854231    159\n",
       "HS17-854232    159\n",
       "HS17-854239    159\n",
       "HS17-854233     65\n",
       "HS17-030299     41\n",
       "              ... \n",
       "HS02-761100      1\n",
       "HS02-761090      1\n",
       "HS02-761010      1\n",
       "HS02-760900      1\n",
       "HS17-999999      1\n",
       "Name: target, Length: 25986, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = return_df2(data, cols)\n",
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09413ed8-b4f9-4e3c-91b6-2b8568351d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>sourceID</th>\n",
       "      <th>targetID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, target, sourceID, targetID]\n",
       "Index: []"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.target==\"HS17-854231\")&(df.source.str.contains(\"HS12-6\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a4b4a9-d5e8-4420-a9ce-5c8c46ad22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return the used versions\n",
    "import functools\n",
    "from datetime import datetime\n",
    "\n",
    "def return_used_versions(start_year = None, end_year=None):\n",
    "    if start_year==None: \n",
    "        start_year = 1992\n",
    "    if end_year==None: \n",
    "        end_year = datetime.now().year\n",
    "    if start_year < 1992 or end_year<start_year or end_year>datetime.now().year: \n",
    "        print(\"Uncorrect years\")\n",
    "    else: \n",
    "        years = [1992, 1996, 2002, 2007, 2012, 2017]\n",
    "        versions = ['HS92', 'HS96', 'HS02', 'HS07', 'HS12', 'HS17']\n",
    "        version_dict = {k:v for k,v in zip(years, versions)}\n",
    "        start_v = functools.reduce(lambda a,b: a if a<=start_year else b, list(reversed(years)))\n",
    "        end_v = functools.reduce(lambda a,b: a if a<=end_year else b, list(reversed(years)))\n",
    "        years_list = years[years.index(start_v):years.index(end_v)+1]\n",
    "        return [version_dict[x] for x in years_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70328cb8-7d1e-4f3e-9374-a5f4d962b49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HS96', 'HS02', 'HS07', 'HS12']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_used_versions(2000,2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5b44bc6-d69f-43c8-91d0-a9098a841d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outer_targets(data, position=None, start_year = None, end_year=None): \n",
    "    versions = return_used_versions(start_year, end_year)\n",
    "    lversion = versions[-1]\n",
    "    rel_ids = df[df.target==lversion+\"-\"+str(position)]['sourceID'].to_list()\n",
    "    return df[df.sourceID.isin(rel_ids)]['target'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5643aef2-3fd2-4fac-96c1-c98a0c2b0a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_outer_targets(df, position=854231, start_year = 2015, end_year= 2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07ebc2c7-0c55-489e-91bb-e99c7eb93328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Graph(object):\n",
    "    \"\"\" Graph data structure, undirected by default. \"\"\"\n",
    "\n",
    "    def __init__(self, connections, directed=False):\n",
    "        self._graph = defaultdict(set)\n",
    "        self._directed = directed\n",
    "        self.add_connections(connections)\n",
    "\n",
    "    def add_connections(self, connections):\n",
    "        \"\"\" Add connections (list of tuple pairs) to graph \"\"\"\n",
    "\n",
    "        for node1, node2 in connections:\n",
    "            self.add(node1, node2)\n",
    "\n",
    "    def add(self, node1, node2):\n",
    "        \"\"\" Add connection between node1 and node2 \"\"\"\n",
    "\n",
    "        self._graph[node1].add(node2)\n",
    "        if not self._directed:\n",
    "            self._graph[node2].add(node1)\n",
    "\n",
    "    def remove(self, node):\n",
    "        \"\"\" Remove all references to node \"\"\"\n",
    "\n",
    "        for n, cxns in self._graph.items():  # python3: items(); python2: iteritems()\n",
    "            try:\n",
    "                cxns.remove(node)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        try:\n",
    "            del self._graph[node]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    def is_connected(self, node1, node2):\n",
    "        \"\"\" Is node1 directly connected to node2 \"\"\"\n",
    "\n",
    "        return node1 in self._graph and node2 in self._graph[node1]\n",
    "\n",
    "    def find_path(self, node1, node2, path=[]):\n",
    "        \"\"\" Find any path between node1 and node2 (may not be shortest) \"\"\"\n",
    "\n",
    "        path = path + [node1]\n",
    "        if node1 == node2:\n",
    "            return path\n",
    "        if node1 not in self._graph:\n",
    "            return None\n",
    "        for node in self._graph[node1]:\n",
    "            if node not in path:\n",
    "                new_path = self.find_path(node, node2, path)\n",
    "                if new_path:\n",
    "                    return new_path\n",
    "        return None\n",
    "\n",
    "    def find_all_connected_nodes(self, qnode, path=[]):\n",
    "        \n",
    "        path = path + [qnode]\n",
    "        if qnode in path[:-1]:\n",
    "            return path\n",
    "        if qnode not in self._graph:\n",
    "            return None\n",
    "        for node in self._graph[qnode]:\n",
    "            print(node)\n",
    "            if node not in path:\n",
    "                new_path = self.find_all_connected_nodes(node, path)\n",
    "                if new_path:\n",
    "                    return new_path\n",
    "        return None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '{}({})'.format(self.__class__.__name__, dict(self._graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9e60a4-5e2a-457c-9c77-2f73903f84fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"./data/CompleteCorrelationsOfHS-SITC-BEC_20170606.xlsx\")\n",
    "cols = ['HS92','HS96','HS02','HS07','HS12','HS17']\n",
    "data = data[cols].dropna().drop_duplicates()\n",
    "for col in cols: \n",
    "    data[col] = data[col].apply(lambda x: col+\"-\"+str(int(x)).zfill(6))\n",
    "connections = []\n",
    "for i in range(len(cols)-1): \n",
    "        temp_tup_list = list(data[[cols[i], cols[i+1]]].drop_duplicates().itertuples(index=False, name=None))\n",
    "        connections = connections + temp_tup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ea0a993-1802-4acd-8b98-dc52853a08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(connections, directed=False)\n",
    "pretty_print = pprint.PrettyPrinter()\n",
    "# pretty_print.pprint(g._graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "975a2295-3761-4bf6-8a72-cc638c99d5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HS92-010111',\n",
      " 'HS96-010111',\n",
      " 'HS02-010110',\n",
      " 'HS07-010110',\n",
      " 'HS12-010130',\n",
      " 'HS07-010190',\n",
      " 'HS02-010190',\n",
      " 'HS96-010120']\n"
     ]
    }
   ],
   "source": [
    "pretty_print.pprint(g.find_path(\"HS92-010111\",'HS96-010120'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b11b307-3340-4590-8e68-fbd192fb5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.find_all_connected_nodes('HS96-010120')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
